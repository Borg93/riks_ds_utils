{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pathlib import PurePath\n",
    "import cv2\n",
    "from riks_ds_utils.page_transforms import PageTransforms\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "from riks_ds_utils.preprocess import Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_page_pairs(basepath):\n",
    "    imgs = glob(os.path.join(basepath, '**'), recursive=True)\n",
    "    page = glob(os.path.join(basepath, '**'), recursive=True)\n",
    "\n",
    "    page = [x for x in imgs if x.lower().endswith(('.xml'))]\n",
    "    imgs = [x for x in imgs if x.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', 'tif','.bmp', '.gif'))]\n",
    "\n",
    "    imgs.sort()\n",
    "\n",
    "    #page_names = [Path(x).stem for x in page]\n",
    "    page_names = ['_'.join(PurePath(x).parts[-3:]).split('.')[0] for x in page]\n",
    "\n",
    "    page_names = [x.replace('_page', '') for x in page_names]\n",
    "    imgs_page = list()\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        img_name = '_'.join(PurePath(img).parts[-2:]).split('.')[0]\n",
    "\n",
    "        #img_name = Path(img).stem\n",
    "    \n",
    "        try:\n",
    "            ind = page_names.index(img_name)\n",
    "            imgs_page.append((imgs[i], page[ind]))    \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return imgs_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the code for conditional file_names if duplicates, also write code for structure the dataset by subpaths (volumes)\n",
    "\n",
    "def binarise_and_write_dataset_structure(img, page, output_path, duplicates):\n",
    "\n",
    "    #if the image names are unique within the dataset\n",
    "    img_stem = Path(img).stem\n",
    "    img_name = Path(img).name\n",
    "    page_name = Path(page).name\n",
    "\n",
    "    #otherwise, use part of the path to separate the images, giving them new unique names\n",
    "    #img_stem = '_'.join(PurePath(img).parts[-2:]).split('.')[0]\n",
    "    #img_name = '_'.join(PurePath(img).parts[-2:])\n",
    "    #page_name = '_'.join(PurePath(page).parts[-3:]).split('.')[0].replace('_page', '')\n",
    "\n",
    "    #police reports separated by volumes as well?\n",
    "\n",
    "    try:\n",
    "        img_ori = cv2.imread(img)\n",
    "        img_gray = cv2.cvtColor(img_ori, cv2.COLOR_BGR2GRAY)\n",
    "        dst = cv2.fastNlMeansDenoising(img_gray, h=31, templateWindowSize=7, searchWindowSize=21)\n",
    "        img_blur = cv2.medianBlur(dst,3).astype('uint8')\n",
    "        threshed = cv2.adaptiveThreshold(img_blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "\n",
    "    os.makedirs(os.path.join(output_path, img_stem), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_path, img_stem, 'bin_image'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_path, img_stem, 'ori_image'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_path, img_stem, 'page'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_path, img_stem, 'text_regions_bin'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_path, img_stem, 'text_regions_ori'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_path, img_stem, 'line_images_bin'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_path, img_stem, 'line_images_ori'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_path, img_stem, 'gt'), exist_ok=True)\n",
    "\n",
    "    cv2.imwrite(os.path.join(output_path, img_stem, 'bin_image', 'bin_' + img_name), threshed)\n",
    "    \n",
    "    shutil.copy(page, os.path.join(output_path, img_stem, 'page', page_name))\n",
    "    shutil.copy(img, os.path.join(output_path, img_stem, 'ori_image', img_name))\n",
    "    #dst_file = os.path.join(output_path, dataset_path,  Path(page).name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_bin_ori_gt(f_w_ori, f_w_bin, ground_truths_ori, ground_truths_bin):\n",
    "    for gt_ori, gt_bin in zip(ground_truths_ori, ground_truths_bin):\n",
    "            line_ori = json.loads(gt_ori)\n",
    "            line_bin = json.loads(gt_bin)\n",
    "            s_ori = json.dumps(line_ori, ensure_ascii=False)\n",
    "            s_bin = json.dumps(line_bin, ensure_ascii=False)\n",
    "            f_w_ori.write(s_ori)\n",
    "            f_w_ori.write('\\n')\n",
    "            f_w_bin.write(s_bin)\n",
    "            f_w_bin.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_and_write_text_rec_gt(basepath: str, split: float):\n",
    "\n",
    "    gts = glob(os.path.join(basepath, '**', 'gt', '*.txt'), recursive=True)\n",
    "    gts_bin = [x for x in gts if 'bin_gt' in x]\n",
    "    gts_ori = [x for x in gts if 'bin_gt' not in x]\n",
    "\n",
    "    assert len(gts_bin) == len(gts_ori)\n",
    "\n",
    "    ground_truths_ori = list()\n",
    "    ground_truths_bin = list()\n",
    "\n",
    "    for gt_ori, gt_bin in zip(gts_ori, gts_bin):\n",
    "        with open(gt_ori, 'r') as f_ori, open(gt_bin, 'r') as f_bin:\n",
    "            ground_truths_ori += f_ori.readlines()\n",
    "            ground_truths_bin += f_bin.readlines()\n",
    "\n",
    "\n",
    "    with open(os.path.join(basepath, 'gt_files', 'text_recognition_all_ori.jsonl'), 'w') as f_w_ori, open(os.path.join(basepath, 'gt_files', 'text_recognition_all_bin.jsonl'), 'w') as f_w_bin:\n",
    "        write_bin_ori_gt(f_w_ori, f_w_bin, ground_truths_ori, ground_truths_bin)\n",
    "\n",
    "    cutoff = int(len(ground_truths_ori) * split)\n",
    "\n",
    "    random.shuffle(ground_truths_ori)\n",
    "    random.shuffle(ground_truths_bin)\n",
    "\n",
    "    val_ori = ground_truths_ori[0:cutoff]\n",
    "    train_ori = ground_truths_ori[cutoff:]\n",
    "    val_bin = ground_truths_bin[0:cutoff]\n",
    "    train_bin = ground_truths_bin[cutoff:]\n",
    "\n",
    "\n",
    "    with open(os.path.join(basepath, 'gt_files', 'text_recognition_ori_train.jsonl'), 'w') as f_w_ori, open(os.path.join(basepath, 'gt_files', 'text_recognition_bin_train.jsonl'), 'w') as f_w_bin:\n",
    "        write_bin_ori_gt(f_w_ori, f_w_bin, train_ori, train_bin)\n",
    "\n",
    "    with open(os.path.join(basepath, 'gt_files', 'text_recognition_ori_val.jsonl'), 'w') as f_w_ori, open(os.path.join(basepath, 'gt_files', 'text_recognition_bin_val.jsonl'), 'w') as f_w_bin:\n",
    "        write_bin_ori_gt(f_w_ori, f_w_bin, val_ori, val_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_duplicate_file_names(imgs):\n",
    "    img_names = [Path(x).stem for x in imgs]\n",
    "    \n",
    "    if len(set(img_names)) == len(img_names):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "        \n",
    "        \"\"\"\n",
    "        img_names = ['_'.join(PurePath(x[0]).parts[-2:]).split('.')[0] for x in imgs_page]\n",
    "        if len(set(img_names)) == len(img_names):\n",
    "            print(len(img_names))\n",
    "            print(len(set(img_names)))\n",
    "            return 'no duplicates when fixed'\n",
    "        else:\n",
    "            return 'still duplicates'\n",
    "    \n",
    "    #img_stem = '_'.join(PurePath(img).parts[-2:]).split('.')[0]\n",
    "    #img_name = '_'.join(PurePath(img).parts[-2:])\n",
    "    #page_name = '_'.join(PurePath(page).parts[-3:]).split('.')[0].replace('_page', '')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binarizing and writing dataset structure\n",
      "5209\n",
      "4178\n",
      "5209\n",
      "5209\n",
      "no duplicates when fixed\n"
     ]
    }
   ],
   "source": [
    "basepath = '/media/erik/Elements/Riksarkivet/data/datasets/htr/HTR_1700_23_09_29'\n",
    "\n",
    "print(\"binarizing and writing dataset structure\")\n",
    "\n",
    "#os.makedirs(basepath, exist_ok=True)\n",
    "\n",
    "imgs_page = get_img_page_pairs('/media/erik/Elements/Riksarkivet/data/datasets/htr/raw/HTR_1700')\n",
    "\n",
    "print(check_for_duplicate_file_names(imgs_page))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binarize and write dataset structure\n",
    "\n",
    "basepath = '/media/erik/Elements/Riksarkivet/data/datasets/htr/Trolldomskommissionen2'\n",
    "\n",
    "print(\"binarizing and writing dataset structure\")\n",
    "\n",
    "basepath = '/media/erik/Elements/Riksarkivet/data/datasets/htr/Trolldomskommissionen2'\n",
    "os.makedirs(basepath, exist_ok=True)\n",
    "\n",
    "imgs_page = get_img_page_pairs('/home/erik/Riksarkivet/Projects/riks_ds_utils/data/Trolldomskommissionen-GT-2023-07-08') #transkribus-export\n",
    "imgs = [x[0] for x in imgs_page]\n",
    "if check_for_duplicate_file_names(imgs):\n",
    "    imgs_new = ['_'.join(PurePath(x).parts[-2:]).split('.')[0] for x in imgs]\n",
    "    duplicates = True\n",
    "    if check_for_duplicate_file_names(imgs_new):\n",
    "        raise Exception('Duplicate file_names and could not be fixed by including part of path, needs to be dealt with manually')\n",
    "else:\n",
    "    duplicates = False\n",
    "\n",
    "args = [(img, page, basepath, duplicates) for img, page in imgs_page]\n",
    "\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "p = Pool(cpu_count)\n",
    "p.starmap(binarise_and_write_dataset_structure, args)\n",
    "\n",
    "#Write coco-file for regions\n",
    "\n",
    "print('Creating coco-file for regions')\n",
    "\n",
    "os.makedirs(os.path.join(basepath, 'gt_files'), exist_ok=True)\n",
    "\n",
    "imgs_bin = glob(os.path.join(basepath, '**', 'bin_image', '**'), recursive=True)\n",
    "imgs_ori = glob(os.path.join(basepath, '**', 'ori_image', '**'), recursive=True)\n",
    "page = glob(os.path.join(basepath, '**', 'page', '**'), recursive=True)\n",
    "imgs_bin = [x.strip() for x in imgs_bin if os.path.isfile(x)]\n",
    "imgs_ori = [x.strip() for x in imgs_ori if os.path.isfile(x)]\n",
    "page = [x for x in page if os.path.isfile(x)]\n",
    "\n",
    "imgs_bin.sort()\n",
    "imgs_ori.sort()\n",
    "page.sort()\n",
    "\n",
    "assert len(imgs_bin) == len(page) == len(imgs_ori)\n",
    "\n",
    "PageTransforms.page_to_region_coco(\n",
    "    xmls=page,\n",
    "    imgs=imgs_ori,\n",
    "    out_path=os.path.join(basepath, 'gt_files', 'coco_regions_ori_TK.json'),\n",
    "    elems=['TextRegion'],\n",
    "    schema='http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15'\n",
    ")\n",
    "\n",
    "PageTransforms.page_to_region_coco(\n",
    "    xmls=page,\n",
    "    imgs=imgs_bin,\n",
    "    out_path=os.path.join(basepath, 'gt_files', 'coco_regions_bin_TK.json'),\n",
    "    elems=['TextRegion'],\n",
    "    schema='http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#Crop text-line images and write a text-recognition gt_file for each page\n",
    "\n",
    "print('Crop text-line images and writing jsonl for text-recognition for each page')\n",
    "\n",
    "args = [\n",
    "    (p, \n",
    "     im, \n",
    "     basepath, \n",
    "     os.path.join(*PurePath(im).parts[0:-2], 'line_images_ori'), \n",
    "     os.path.join(*PurePath(p).parts[0:-2], 'gt'), \n",
    "     'http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15', \n",
    "     False) for p, im in zip(page, imgs_ori)\n",
    "     ]\n",
    "\n",
    "args_bin = [\n",
    "    (p, \n",
    "     im, \n",
    "     basepath, \n",
    "     os.path.join(*PurePath(im).parts[0:-2], 'line_images_bin'), \n",
    "     os.path.join(*PurePath(p).parts[0:-2], 'gt'), \n",
    "     'http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15', \n",
    "     True) for p, im in zip(page, imgs_bin)\n",
    "     ]\n",
    "\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "p = Pool(cpu_count)\n",
    "p.starmap(PageTransforms.crop_line_imgs_page, args)\n",
    "p.starmap(PageTransforms.crop_line_imgs_page, args_bin)\n",
    "\n",
    "#Collect all gt_files and write combined gt_files for text-recognition\n",
    "\n",
    "print('Collect all gt_files and write combined gt_files for text-recognition')\n",
    "\n",
    "collect_and_write_text_rec_gt(basepath, 0.1)\n",
    "\n",
    "#Crop regions and write coco_lines (images in coco are all the cropped regions, line coords are translated with respect to these regions)\n",
    "\n",
    "print('Crop regions and write coco_lines')\n",
    "\n",
    "PageTransforms.crop_text_reg_write_text_line_coco(page, imgs_bin, os.path.join(basepath, 'gt_files', 'coco_lines_bin_TK.json'), binarized=True)\n",
    "PageTransforms.crop_text_reg_write_text_line_coco(page, imgs_ori, os.path.join(basepath, 'gt_files', 'coco_lines_ori_TK.json'), binarized=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "basepath = '/media/erik/Elements/Riksarkivet/data/datasets/htr/Trolldomskommissionen'\n",
    "\n",
    "imgs = glob(os.path.join(basepath, '**', 'bin_image', '**'), recursive=True)\n",
    "page = glob(os.path.join(basepath, '**', 'page', '**'), recursive=True)\n",
    "imgs = [x for x in imgs if os.path.isfile(x)]\n",
    "page = [x for x in page if os.path.isfile(x)]\n",
    "\n",
    "imgs.sort()\n",
    "page.sort()\n",
    "\n",
    "assert len(imgs) == len(page)\n",
    "\n",
    "#imgs_checked = glob(os.path.join('/media/erik/Elements/Riksarkivet/data/datasets/htr/segmentation/ICDAR-2019/fiftyone/data', '**'))\n",
    "#imgs_checked = [Path(x).name for x in imgs_checked]\n",
    "\n",
    "imgs_page = list(zip(imgs, page))\n",
    "\n",
    "print(len(imgs_page))\n",
    "\n",
    "#imgs_page = [(x, y) for (x, y) in imgs_page if Path(x).name in imgs_checked]\n",
    "\n",
    "#print(len(imgs_page))\n",
    "\n",
    "imgs = [x for (x, y) in imgs_page]\n",
    "page = [y for (x, y) in imgs_page]\n",
    "\n",
    "PageTransforms.page_to_region_coco(\n",
    "    xmls=page,\n",
    "    imgs=imgs,\n",
    "    out_path='/media/erik/Elements/Riksarkivet/data/datasets/htr/Trolldomskommissionen/gt_files/coco_regions_TK.json',\n",
    "    elems=['TextRegion'],\n",
    "    schema='http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780\n",
      "780\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "min() arg is an empty sequence\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "basepath = '/media/erik/Elements/Riksarkivet/data/datasets/htr/Trolldomskommissionen'\n",
    "\n",
    "imgs = glob(os.path.join(basepath, '**', 'bin_image', '**'), recursive=True)\n",
    "page = glob(os.path.join(basepath, '**', 'page', '**'), recursive=True)\n",
    "imgs = [x for x in imgs if os.path.isfile(x)]\n",
    "page = [x for x in page if os.path.isfile(x)]\n",
    "\n",
    "imgs.sort()\n",
    "page.sort()\n",
    "\n",
    "\n",
    "\n",
    "print(len(imgs))\n",
    "print(len(page))\n",
    "\n",
    "# imgs = [os.path.sep.join(x.split(os.path.sep)[-3:]) for x in imgs]\n",
    "\n",
    "\"\"\"\n",
    "for img in imgs:\n",
    "    dst = os.path.join('/media/erik/T7/Data/Text_line_segmentation/ICDAR-2019/fiftyone/data', Path(img).name)\n",
    "    shutil.copy(img, dst)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PageTransforms.page_to_region_coco(\n",
    "    xmls=page,\n",
    "    imgs=imgs,\n",
    "    out_path='/media/erik/Elements/Riksarkivet/data/datasets/htr/segmentation/police_records/gt_files/coco_regions.json',\n",
    "    elems=['TextRegion'],\n",
    "    schema='http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "args = [(p, im, basepath, os.path.join(*PurePath(im).parts[0:-2], 'line_images'), os.path.join(*PurePath(p).parts[0:-2], 'gt'), 'http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15') for p, im in zip(page, imgs)]\n",
    "\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "p = Pool(cpu_count)\n",
    "p.starmap(PageTransforms.crop_line_imgs_page, args)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "PageTransforms.crop_text_reg_write_text_line_coco(page, imgs, os.path.join(basepath, 'gt_files', 'coco_lines_TK.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gts = glob(os.path.join('/media/erik/Elements/Riksarkivet/data/datasets/htr/Trolldomskommissionen', '**', 'gt', '*.txt'), recursive=True)\n",
    "\n",
    "ground_truths = list()\n",
    "\n",
    "for gt in gts:\n",
    "    with open(gt, 'r') as f:\n",
    "        ground_truths += f.readlines()\n",
    "\n",
    "\n",
    "with open('/media/erik/Elements/Riksarkivet/data/datasets/htr/Trolldomskommissionen/gt_files/text_recognition_all.jsonl', 'w') as f:\n",
    "    for gt in ground_truths:\n",
    "        line = json.loads(gt)\n",
    "        s = json.dumps(line, ensure_ascii=False)\n",
    "        f.write(s)\n",
    "        f.write('\\n')\n",
    "\n",
    "cutoff = int(len(ground_truths) * 0.1)\n",
    "\n",
    "random.shuffle(ground_truths)\n",
    "\n",
    "val = ground_truths[0:cutoff]\n",
    "train = ground_truths[cutoff:]\n",
    "\n",
    "\n",
    "with open('/media/erik/Elements/Riksarkivet/data/datasets/htr/Trolldomskommissionen/gt_files/text_recognition_train.jsonl', 'w') as f:\n",
    "    for gt in train:\n",
    "        line = json.loads(gt)\n",
    "        s = json.dumps(line, ensure_ascii=False)\n",
    "        f.write(s)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('/media/erik/Elements/Riksarkivet/data/datasets/htr/Trolldomskommissionen/gt_files/text_recognition_val.jsonl', 'w') as f:\n",
    "    for gt in val:\n",
    "        line = json.loads(gt)\n",
    "        s = json.dumps(line, ensure_ascii=False)\n",
    "        f.write(s)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PageTransforms.crop_line_imgs_page(\n",
    "    image='/home/erik/Riksarkivet/Data/HTR/HTR_1700_clean/images/Bergskollegium_E3_10_(1718-1727)*40004031_00007.tif',\n",
    "    page_file='/home/erik/Riksarkivet/Data/HTR/HTR_1700_clean/page/Bergskollegium_E3_10_(1718-1727)*40004031_00007.xml',\n",
    "    schema='',\n",
    "    output_base_path='/home/erik/Riksarkivet/Data/HTR/HTR_1700_clean',\n",
    "    separator='*'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PageTransforms.page_to_mmlabs_ocr(\n",
    "    page_path='/home/erik/Riksarkivet/Data/HTR/HTR_1700_clean/page',\n",
    "    imgs_path='/home/erik/Riksarkivet/Data/HTR/HTR_1700_clean/images',\n",
    "    out_path='/home/erik/Riksarkivet/Projects/riks_ds_utils/data/processed/OCRDataset2_1700.json',\n",
    "    schema='http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from riks_ds_utils.mmlabs_utils import MMLabsUtils\n",
    "\n",
    "train, val = MMLabsUtils.split_ocr_dataset('/home/erik/Riksarkivet/Projects/riks_ds_utils/data/processed/OCRDataset2_1700.json', 0.1)\n",
    "\n",
    "PageTransforms._write_json('/home/erik/Riksarkivet/Projects/riks_ds_utils/data/processed/OCRDataset2_1700_train.json', train)\n",
    "PageTransforms._write_json('/home/erik/Riksarkivet/Projects/riks_ds_utils/data/processed/OCRDataset2_1700_val.json', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = PageTransforms.extract_dict_from_page('/home/erik/Riksarkivet/Data/HTR/HTR_1700_clean/page', 'http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/erik/Riksarkivet/Projects/riks_ds_utils/data/processed/dict1700.txt', 'w') as f:\n",
    "    for char in dict:\n",
    "        f.write(char + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (img, page) in enumerate(imgs_page):\n",
    "    im = cv2.imread(img)\n",
    "    if im is None:\n",
    "        print('a')\n",
    "    else:\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
